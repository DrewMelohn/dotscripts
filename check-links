#!/usr/bin/env python3

# PARTNERS WITH DYLAN

import requests, shutil, os
from bs4 import BeautifulSoup
from sys import argv
from pathlib import Path

resp = requests.get(argv[-1])
soup = BeautifulSoup(resp.text, features="html.parser")
links = soup.find_all('a')

links_broken = []

bad = Path("/home/meloand/.links/bad-links.txt")
dir_path = Path("/home/meloand/.links/")

for result in links:
    soab = str(result).split('"')[1]
    if "http" not in result:
        test = requests.get(argv[-1] + soab)
    else:
        test = request.get(result)
    if test.status_code == 404:
        links_broken.append(result)

if bad.is_file() and not links_broken:
    shutil.rmtree(dir_path )
elif links_broken:
    if not dir_path.exists():
        os.mkdir(dir_path)
    with open(bad, 'w') as txt:
        for link in links_broken:
            txt.write(str(link) +"\n")

